{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# from rl_agents.env_utils import rollouts_generator, get_adv_vtarg, get_gaeadv_vtarg\n",
    "from rl_agents.vpg.agent import VPG_Agent\n",
    "from rl_agents.ppo.agent import PPO_Agent\n",
    "from rl_agents.training.buffers import GAE_Buffer\n",
    "from rl_agents.training.sensei import Sensei, ExperimentRunner\n",
    "from rl_agents.utils import get_actor_critic, simple_run, Logger\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GYM environment\n",
    "Use Pendulum-v0 for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_fn = lambda: gym.make('MountainCarContinuous-v0')\n",
    "env_fn = lambda: gym.make('Pendulum-v0')\n",
    "# env_fn = lambda: gym.make('MountainCar-v0')\n",
    "# env_fn = lambda: gym.make('CartPole-v0')\n",
    "# env_fn = lambda: gym.make('LunarLanderContinuous-v2')\n",
    "# env_fn = lambda: gym.make('Acrobot-v1')\n",
    "\n",
    "env = env_fn()\n",
    "obs_dim = env.observation_space.shape\n",
    "act_dim = env.action_space.shape or env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.95\n",
    "gamma = 0.99\n",
    "buffer_size = 4096\n",
    "\n",
    "logger = Logger(env.unwrapped.spec.id)\n",
    "\n",
    "buff_vpg = GAE_Buffer(obs_dim, act_dim, buffer_size, gamma=gamma, lam=lam)\n",
    "actor_vpg, critic_vpg = get_actor_critic(env)\n",
    "jen_vpg = VPG_Agent(actor_vpg, critic_vpg, logger=logger)\n",
    "\n",
    "epochs_actor = 1\n",
    "epochs_critic = 80\n",
    "sensei_vpg = Sensei(jen_vpg, env_fn, buff_vpg,\n",
    "                    epochs_actor=epochs_actor, epochs_critic=epochs_critic,\n",
    "                    logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer Actor is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer Critic is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "-329.29993\n",
      "-384.77612\n",
      "-431.63824\n",
      "-472.97665\n",
      "-524.8154\n",
      "-491.02197\n",
      "-548.6677\n",
      "-613.15674\n",
      "-562.34515\n",
      "-516.8168\n",
      "-547.1013\n",
      "-527.65643\n",
      "-536.3221\n",
      "-522.0013\n",
      "-646.14966\n",
      "-533.3036\n",
      "-646.97394\n",
      "-522.60297\n",
      "-569.0261\n",
      "-463.31598\n",
      "-496.78296\n",
      "-520.1948\n",
      "-529.9783\n",
      "-563.0763\n",
      "-512.5211\n",
      "-577.4538\n",
      "-624.0139\n",
      "-628.26776\n",
      "-553.6127\n",
      "-522.5342\n",
      "-507.23932\n",
      "-499.2244\n",
      "-491.99146\n",
      "-511.5661\n",
      "-497.34113\n",
      "-484.82297\n",
      "-496.7782\n",
      "-486.9269\n",
      "-486.38123\n",
      "-554.1598\n",
      "-511.16672\n",
      "-515.12146\n",
      "-521.5029\n",
      "-516.624\n",
      "-482.068\n",
      "-491.98816\n",
      "-514.364\n",
      "-484.1015\n",
      "-493.41925\n",
      "-519.7002\n",
      "-617.8916\n",
      "-683.5029\n",
      "-644.3469\n",
      "-588.0858\n",
      "-497.40512\n",
      "-482.5019\n",
      "-498.87396\n",
      "-513.5836\n",
      "-496.18433\n",
      "-491.3198\n",
      "-509.74567\n",
      "-465.59308\n",
      "-502.14697\n",
      "-510.87363\n",
      "-555.9824\n",
      "-494.60416\n",
      "-489.75763\n",
      "-482.95282\n",
      "-494.53903\n",
      "-457.71442\n",
      "-485.37622\n",
      "-549.5299\n",
      "-489.10037\n",
      "-481.62698\n",
      "-484.70996\n",
      "-509.55304\n",
      "-478.6583\n",
      "-498.22827\n",
      "-458.9801\n",
      "-455.71887\n",
      "-454.03082\n",
      "-505.06815\n",
      "-549.52246\n",
      "-569.21277\n",
      "-507.91733\n",
      "-570.2495\n",
      "-484.18204\n",
      "-509.74478\n",
      "-472.3664\n",
      "-477.9005\n",
      "-546.3457\n",
      "-560.0093\n",
      "-547.9638\n",
      "-566.11646\n",
      "-518.9872\n",
      "-487.55997\n",
      "-466.60437\n",
      "-495.81363\n",
      "-516.3635\n",
      "-550.3301\n",
      "-518.1541\n",
      "-548.4441\n",
      "-531.2101\n",
      "-514.37103\n",
      "-545.70544\n",
      "-502.75543\n",
      "-532.43384\n",
      "-522.431\n",
      "-528.193\n",
      "-464.8017\n",
      "-464.76712\n",
      "-464.678\n",
      "-531.0375\n",
      "-448.87885\n",
      "-496.60944\n",
      "-474.03027\n",
      "-497.54236\n",
      "-473.84415\n",
      "-575.553\n",
      "-532.38556\n",
      "-530.48737\n",
      "-498.21854\n",
      "-459.73007\n",
      "-466.2034\n",
      "-526.8047\n",
      "-534.7184\n",
      "-521.7273\n",
      "-528.1406\n",
      "-466.87506\n",
      "-542.8018\n",
      "-598.7003\n",
      "-582.28467\n",
      "-582.276\n",
      "-597.2408\n",
      "-595.8001\n",
      "-621.1959\n",
      "-625.3546\n",
      "-626.68164\n",
      "-644.5743\n",
      "-633.62384\n",
      "-627.8901\n",
      "-626.5333\n",
      "-609.50854\n",
      "-636.28156\n",
      "-616.40643\n",
      "-617.99426\n",
      "-637.12146\n",
      "-663.9972\n",
      "-666.1654\n",
      "-687.2884\n",
      "-670.6981\n",
      "-646.3007\n",
      "-647.5834\n",
      "-652.1883\n",
      "-657.02185\n",
      "-685.4487\n",
      "-662.8752\n",
      "-655.18896\n",
      "-641.36707\n",
      "-644.62213\n",
      "-680.4076\n",
      "-672.30444\n",
      "-715.83746\n",
      "-724.3785\n",
      "-741.3777\n",
      "-744.3512\n",
      "-752.46606\n",
      "-751.4183\n",
      "-743.44116\n",
      "-725.76416\n",
      "-721.38184\n",
      "-671.1853\n",
      "-703.6256\n",
      "-680.4171\n",
      "-687.6154\n",
      "-674.6515\n",
      "-651.53674\n",
      "-650.429\n",
      "-664.38\n",
      "-664.3059\n",
      "-675.004\n",
      "-704.64844\n",
      "-678.4072\n",
      "-692.6714\n",
      "-664.2599\n",
      "-674.4335\n",
      "-668.8097\n",
      "-673.8199\n",
      "-688.86255\n",
      "-677.4263\n",
      "-686.8788\n",
      "-671.1456\n",
      "-673.8237\n",
      "-672.216\n",
      "-671.69543\n",
      "-687.64716\n",
      "-679.28467\n",
      "-708.6237\n",
      "-688.4282\n",
      "-689.27136\n"
     ]
    }
   ],
   "source": [
    "num_ite = 200\n",
    "sensei_vpg.train(num_ite, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "buff_ppo = GAE_Buffer(obs_dim, act_dim, 2048, gamma=0.99, lam=0.95)\n",
    "actor_ppo, critic_ppo = get_actor_critic(env)\n",
    "jen_ppo = PPO_Agent(actor_ppo, critic_ppo, act_dim)\n",
    "\n",
    "lam = 0.95\n",
    "gamma = 0.99\n",
    "epochs_actor = 10\n",
    "epochs_critic = 80\n",
    "sensei_ppo = Sensei(jen_ppo, env_fn, buff_ppo,\n",
    "                    epochs_actor=epochs_actor, epochs_critic=epochs_critic,\n",
    "                    gamma=gamma, gae_lambda=lam,\n",
    "                    log_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer Actor is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer Critic is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "0 20.63917525773196\n",
      "1 9.538812785388128\n",
      "2 9.460829493087557\n",
      "3 9.34703196347032\n",
      "4 9.290909090909091\n",
      "5 9.331818181818182\n",
      "6 9.333333333333334\n",
      "7 9.313636363636364\n",
      "8 9.351598173515981\n",
      "9 9.337899543378995\n",
      "10 9.378995433789955\n",
      "11 9.444444444444445\n",
      "12 9.285067873303168\n",
      "13 9.371559633027523\n",
      "14 25.65\n",
      "15 21.49473684210526\n",
      "16 15.694656488549619\n",
      "17 16.983193277310924\n",
      "18 15.930232558139535\n",
      "19 14.16551724137931\n",
      "20 16.75409836065574\n",
      "21 13.825503355704697\n",
      "22 17.435897435897434\n",
      "23 13.552631578947368\n",
      "24 17.19327731092437\n",
      "25 18.90740740740741\n",
      "26 22.98876404494382\n",
      "27 25.024390243902438\n",
      "28 22.32967032967033\n",
      "29 21.842105263157894\n",
      "30 20.48\n",
      "31 20.724489795918366\n",
      "32 20.61\n",
      "33 20.515151515151516\n",
      "34 20.386138613861387\n",
      "35 20.636363636363637\n",
      "36 20.176470588235293\n",
      "37 21.34375\n",
      "38 20.5\n",
      "39 20.07920792079208\n",
      "40 20.696969696969695\n",
      "41 20.77777777777778\n",
      "42 20.737373737373737\n",
      "43 20.0\n",
      "44 20.61\n",
      "45 20.8265306122449\n",
      "46 20.636363636363637\n",
      "47 20.15686274509804\n",
      "48 20.545454545454547\n",
      "49 20.636363636363637\n",
      "50 20.0\n",
      "51 20.346534653465348\n",
      "52 19.776699029126213\n",
      "53 20.396039603960396\n",
      "54 20.27\n",
      "55 20.346534653465348\n",
      "56 20.465346534653467\n",
      "57 20.07843137254902\n",
      "58 20.41\n",
      "59 20.707070707070706\n",
      "60 20.636363636363637\n",
      "61 19.78846153846154\n",
      "62 20.07920792079208\n",
      "63 19.807692307692307\n",
      "64 20.755102040816325\n",
      "65 20.019417475728154\n",
      "66 20.696969696969695\n",
      "67 20.247524752475247\n",
      "68 21.04123711340206\n",
      "69 20.77777777777778\n",
      "70 20.42\n",
      "71 19.78846153846154\n",
      "72 20.4\n",
      "73 19.576923076923077\n",
      "74 20.465346534653467\n",
      "75 20.785714285714285\n",
      "76 21.144329896907216\n",
      "77 20.376237623762375\n",
      "78 19.95098039215686\n",
      "79 19.65714285714286\n",
      "80 21.25\n",
      "81 20.767676767676768\n",
      "82 20.257425742574256\n",
      "83 20.35\n",
      "84 21.237113402061855\n",
      "85 20.656565656565657\n",
      "86 23.837209302325583\n",
      "87 26.115384615384617\n",
      "88 25.0\n",
      "89 22.118279569892472\n",
      "90 21.473684210526315\n",
      "91 20.836734693877553\n",
      "92 21.257731958762886\n",
      "93 20.42\n",
      "94 20.636363636363637\n",
      "95 20.757575757575758\n",
      "96 20.15686274509804\n",
      "97 20.8265306122449\n",
      "98 20.918367346938776\n",
      "99 20.287128712871286\n",
      "100 21.53684210526316\n",
      "101 21.061855670103093\n",
      "102 20.887755102040817\n",
      "103 20.36\n",
      "104 20.77777777777778\n",
      "105 20.425742574257427\n",
      "106 20.128712871287128\n",
      "107 20.63\n",
      "108 21.072164948453608\n",
      "109 20.464646464646464\n",
      "110 21.395833333333332\n",
      "111 20.366336633663366\n",
      "112 20.366336633663366\n",
      "113 20.646464646464647\n",
      "114 19.813725490196077\n",
      "115 20.02912621359223\n",
      "116 22.217391304347824\n",
      "117 39.80769230769231\n",
      "118 123.6875\n",
      "119 120.4375\n",
      "120 147.06666666666666\n",
      "121 76.25925925925925\n",
      "122 70.72413793103448\n",
      "123 49.73170731707317\n",
      "124 47.23255813953488\n",
      "125 46.40909090909091\n",
      "126 42.875\n",
      "127 44.80434782608695\n",
      "128 51.325\n",
      "129 52.282051282051285\n",
      "130 57.083333333333336\n",
      "131 46.93181818181818\n",
      "132 43.148936170212764\n",
      "133 40.509803921568626\n",
      "134 35.8421052631579\n",
      "135 32.70967741935484\n",
      "136 28.95774647887324\n",
      "137 27.6\n",
      "138 27.445945945945947\n",
      "139 28.19178082191781\n",
      "140 26.88157894736842\n",
      "141 26.363636363636363\n",
      "142 26.329113924050635\n",
      "143 26.789473684210527\n",
      "144 26.243589743589745\n",
      "145 26.68831168831169\n",
      "146 27.253333333333334\n",
      "147 26.037974683544302\n",
      "148 26.763157894736842\n",
      "149 27.293333333333333\n",
      "150 26.333333333333332\n",
      "151 26.063291139240505\n",
      "152 28.041095890410958\n",
      "153 31.338461538461537\n",
      "154 32.63492063492063\n",
      "155 33.95\n",
      "156 31.96875\n",
      "157 35.21052631578947\n",
      "158 35.52542372881356\n",
      "159 34.86440677966102\n",
      "160 34.15\n",
      "161 35.0\n",
      "162 32.375\n",
      "163 31.784615384615385\n",
      "164 33.0\n",
      "165 29.028169014084508\n",
      "166 32.61290322580645\n",
      "167 31.166666666666668\n",
      "168 31.12121212121212\n",
      "169 29.782608695652176\n",
      "170 31.890625\n",
      "171 29.1\n",
      "172 28.708333333333332\n",
      "173 28.87323943661972\n",
      "174 28.985714285714284\n",
      "175 30.71641791044776\n",
      "176 30.70149253731343\n",
      "177 27.89041095890411\n",
      "178 27.60810810810811\n",
      "179 27.56756756756757\n",
      "180 29.897058823529413\n",
      "181 28.561643835616437\n",
      "182 28.830985915492956\n",
      "183 28.901408450704224\n",
      "184 28.054794520547944\n",
      "185 27.675675675675677\n",
      "186 28.375\n",
      "187 27.12\n",
      "188 27.493333333333332\n",
      "189 27.513513513513512\n",
      "190 27.56\n",
      "191 27.513513513513512\n",
      "192 28.802816901408452\n",
      "193 27.453333333333333\n",
      "194 27.931506849315067\n",
      "195 27.89041095890411\n",
      "196 27.026315789473685\n",
      "197 27.72972972972973\n",
      "198 29.04225352112676\n",
      "199 27.14666666666667\n"
     ]
    }
   ],
   "source": [
    "num_ite = 200\n",
    "sensei_ppo.train(num_ite, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.keras.optimizers.Adam(3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buff = GAE_Buffer(obs_dim, act_dim, 20, gamma=0.99, lam=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_test, critic_test = get_actor_critic(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vpg = VPG_Agent(actor_test, critic_tes, act_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_runner = ExperimentRunner(test_vpg, env, buff)\n",
    "test_runner.num_ite = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, rollout in enumerate(test_runner):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "obs_dim = env.observation_space.shape\n",
    "act_dim = env.action_space.shape or env.action_space.n\n",
    "\n",
    "actor_vpg, critic_vpg = get_actor_critic(env)\n",
    "jen_vpg = VPG_Agent(actor_vpg, critic_vpg, act_dim)\n",
    "\n",
    "simple_run(env, jen_vpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buff_ppo = GAE_Buffer(obs_dim, act_dim, 2048, gamma=0.99, lam=0.95)\n",
    "actor_ppo, critic_ppo = get_actor_critic(env)\n",
    "jen_ppo = PPO_Agent(actor_ppo, critic_ppo, act_dim)\n",
    "\n",
    "lam = 0.95\n",
    "gamma = 0.99\n",
    "epochs_actor = 10\n",
    "epochs_critic = 80\n",
    "sensei_ppo = Sensei(jen_ppo, env_fn, buff_ppo,\n",
    "                    epochs_actor=epochs_actor, epochs_critic=epochs_critic,\n",
    "                    gamma=gamma, gae_lambda=lam,\n",
    "                    log_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ite = 200\n",
    "sensei_ppo.train(num_ite, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_vpg = GaussianActor(obs_dim, act_dim) if is_continuous else CategoricalActor(obs_dim, act_dim)\n",
    "critic_vpg = Critic(obs_dim)\n",
    "jen_vpg = VPG_Agent(actor_vpg, critic_vpg, is_continuous, act_dim)\n",
    "generator_vpg = rollouts_generator(jen_vpg, env, is_continuous, horizon=2048)\n",
    "\n",
    "alg_name = \"VPG\"\n",
    "lam = 0.95\n",
    "gamma = 0.99\n",
    "epochs_actor = 1\n",
    "epochs_critic = 40\n",
    "sensei_vpg = Sensei(jen_vpg, alg_name, env_fn,\n",
    "                    horizon=2048, epochs_actor=epochs_actor, epochs_critic=epochs_critic,\n",
    "                    gamma=gamma, gae_lambda=lam,\n",
    "                    log_dir='logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ite = 100\n",
    "sensei_vpg.train(num_ite, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_ite = 50\n",
    "sensei_vpg.train(num_ite, record=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = generator_ppo.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae, td = get_gaeadv_vtarg(rollout, 0.95, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rews, vals = rollout[\"rew\"][:198], rollout[\"vpred\"][:198]\n",
    "rews = np.append(rews, rollout[\"vpred\"])\n",
    "vals = np.append(vals, rollout[\"next_vpred\"])\n",
    "deltas = rews[:-1] + 0.99 * vals[1:] - vals[:-1]\n",
    "gae2 = discount_cumsum(deltas, 0.99*0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(gae2 - gae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae-gae2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout[\"new\"][:198]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jen_vpg.actor_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([1,2])\n",
    "b[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = np.array([[-5, 5], [-10, 10]])\n",
    "ac = np.array([2, -19])\n",
    "\n",
    "print(limits[0,:])\n",
    "np.clip(ac, limits[:, 0], limits[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers as kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSample(kl.Layer):\n",
    "    def __init__(self, action_dim):\n",
    "        super(GaussianSample, self).__init__(name='GaussianSample')\n",
    "\n",
    "        # s_init = tf.constant_initializer(np.exp(log_std))\n",
    "        log_std = -0.53 * np.ones(action_dim, dtype=np.float32)\n",
    "        self.log_std = tf.Variable(initial_value=log_std,\n",
    "                               name='log_std', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # If training return dist, else not\n",
    "        # So better to always return everything\n",
    "        # std = tf.zeros_like(inputs) + self.std\n",
    "        return distributions.Normal(loc=inputs, scale=tf.exp(self.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(obs_dim: int, act_dim: int):\n",
    "    model = tf.keras.Sequential([\n",
    "        kl.Dense(32, input_shape=(obs_dim,), activation=tf.keras.activations.tanh),\n",
    "        kl.Dense(act_dim),\n",
    "        GaussianSample(act_dim),\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt_fn(model, lr=3e-4):\n",
    "    opt = tf.keras.optimizers.Adam(lr)\n",
    "    \n",
    "    @tf.function\n",
    "    def step_fn(obs_no, act_na, adv_n):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logp = tf.reduce_sum(\n",
    "                model(obs_no[None]).log_prob(act_na),\n",
    "                axis=1,\n",
    "            )\n",
    "            \n",
    "            loss = tf.reduce_mean(-logp * adv_n)\n",
    "        \n",
    "        grad = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grad, model.trainable_variables))\n",
    "        \n",
    "    return opt, setp_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, epochs=200, buffer_size=4096):\n",
    "    assert isinstance(env.observation_space, Box), \\\n",
    "        \"This is only for continuous observation space\"\n",
    "    assert isinstance(env.action_space, Box), \\\n",
    "        \"This is only for continuous action space\"\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    \n",
    "    model = get_actor(obs_dim, act_dim)\n",
    "    \n",
    "    opt, step_fn = get_opt_fn(model)\n",
    "    \n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for reward-to-go weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "            # save obs\n",
    "            batch_obs.append(obs.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            act = model(obs[None]).sample()[0]\n",
    "            obs, rew, done, _ = env.step(act)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a_t|s_t) is reward-to-go from t\n",
    "                batch_weights += list(reward_to_go(ep_rews))\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > buffer_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        step_fn(batch_obs, batch_acts, batch_weights)\n",
    "        return batch_rets, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tensorflow_probability import distributions as tfpd, layers as tfpl\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSample(kl.Layer):\n",
    "    def __init__(self, act_dim):\n",
    "        super(GaussianSample, self).__init__()\n",
    "        self.log_std = self.add_weight(\n",
    "            'log_std', initializer=tf.keras.initializers.Constant(-0.53), \n",
    "            shape=(act_dim,), trainable=True\n",
    "        )\n",
    "        self.normal_dist = tfpl.DistributionLambda(\n",
    "            make_distribution_fn=lambda t: tfpd.Normal(loc=t, scale=tf.exp(self.log_std)),\n",
    "            convert_to_tensor_fn=lambda s: s.sample(),\n",
    "        )\n",
    "    \n",
    "    #def build(self, input_shape):\n",
    "    #    \"\"\"\n",
    "    #    input_shape: might be [None, act_dim]\n",
    "    #    \"\"\"\n",
    "    #    self.log_std = self.add_weight(\n",
    "    #        'log_std', initializer=tf.keras.initializers.Constant(-0.53), \n",
    "    #        shape=(input_shape[1],), dtype='float32', trainable=True\n",
    "    #    )\n",
    "\n",
    "    def call(self, input):\n",
    "        #return tfpd.Normal(loc=input, scale=tf.exp(self.log_std))\n",
    "        return self.normal_dist(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_actor(obs_dim: int, act_dim: int):\n",
    "    obs = keras.Input(shape=(obs_dim,), name='observations')\n",
    "    x = kl.Dense(32, activation='tanh', name='dense_1')(obs)\n",
    "    x = kl.Dense(act_dim, name='logits')(x)\n",
    "\n",
    "    pi = GaussianSample(act_dim)(x)\n",
    "    \n",
    "    model = keras.Model(inputs=obs,\n",
    "                        outputs=[pi, x])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (distribution_lambda), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'log_std:0' shape=(1,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    }
   ],
   "source": [
    "test_model = get_actor(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.layers[3].log_std.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "[-0.58579165]\n"
     ]
    }
   ],
   "source": [
    "obs = np.array([[ 0.69950885,  0.6432279 , -7.588761  ]])\n",
    "res = test_model(obs)\n",
    "print(res[0].sample()[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tfpd.Categorical(logits=[[2, 2, 3], [2, -2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5514447, -0.3181754], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.log_prob([2, 2]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_actor(obs_dim: int, act_dim: int):\n",
    "    obs = keras.Input(shape=(obs_dim,), name='observations')\n",
    "    x = kl.Dense(32, activation='tanh', name='dense_1')(obs)\n",
    "    x = kl.Dense(act_dim, name='logits')(x)\n",
    "\n",
    "    pi = tfpl.DistributionLambda(\n",
    "        make_distribution_fn=lambda t: tfpd.Categorical(logits=t),\n",
    "        convert_to_tensor_fn=lambda s: s.sample(),\n",
    "    )(x)\n",
    "    \n",
    "    model = keras.Model(inputs=obs,\n",
    "                        outputs=[pi, x])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = get_discrete_actor(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tf.Tensor([-0.9112284], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "obs = np.array([[ 0.69950885,  0.6432279 , -1.588761  ]])\n",
    "res = test_model(obs)\n",
    "print(res[0].sample()[0].numpy())\n",
    "print(res[0].log_prob([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration test model + environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_in_env(model, env):\n",
    "    obs = env.reset()\n",
    "    ret = 0\n",
    "    while True:\n",
    "        act = model(obs[None])[0].sample()[0].numpy()\n",
    "        obs, rew, done, _ = env.step(act)\n",
    "\n",
    "        ret += rew\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Overall return 18.0\n"
     ]
    }
   ],
   "source": [
    "cartpole_env = gym.make('CartPole-v0')\n",
    "\n",
    "model = get_discrete_actor(\n",
    "    discrete_env.observation_space.shape[0], \n",
    "    discrete_env.action_space.n\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall return 12.0\n"
     ]
    }
   ],
   "source": [
    "ret = test_model_in_env(model, cartpole_env)\n",
    "print(f'Overall return {ret}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt_fn(model, lr=3e-4):\n",
    "    opt = tf.keras.optimizers.Adam(lr)\n",
    "    \n",
    "    #@tf.function\n",
    "    def step_fn(obs_no, act_na, adv_n):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logp = model(obs_no)[0].log_prob(act_na)\n",
    "            print(logp.shape)\n",
    "            if len(logp.shape) > 1:\n",
    "                lop =  tf.reduce_sum(logp, axis=1)\n",
    "            \n",
    "            #print(logp.dtype, adv_n.dtype)\n",
    "            loss = - tf.reduce_mean(logp * adv_n)\n",
    "        \n",
    "        grad = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grad, model.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    return opt, step_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_to_go(rews):\n",
    "    n = len(rews)\n",
    "    rtgs = np.zeros_like(rews)\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "    return rtgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, epochs=200, buffer_size=4096):\n",
    "    assert isinstance(env.observation_space, gym.spaces.Box), \\\n",
    "        \"This is only for continuous observation space\"\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    model = None\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        model = get_continuous_actor(obs_dim, env.action_space.shape[0])\n",
    "    else:\n",
    "        model = get_discrete_actor(obs_dim, env.action_space.n)\n",
    "\n",
    "    model.summary()\n",
    "    # keras.utils.plot_model(model, 'multi_output_model.png')\n",
    "    \n",
    "    \n",
    "    opt, step_fn = get_opt_fn(model)\n",
    "    \n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for reward-to-go weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "            # save obs\n",
    "            batch_obs.append(obs.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            act = model(obs[None])[0].sample()[0].numpy()\n",
    "            obs, rew, done, _ = env.step(act)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a_t|s_t) is reward-to-go from t\n",
    "                batch_weights += list(reward_to_go(ep_rews))\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > buffer_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        batch_loss = step_fn(np.array(batch_obs).astype('float32'), \n",
    "                             np.array(batch_acts).astype('float32'), \n",
    "                             np.array(batch_weights).astype('float32'))\n",
    "        return batch_rets, batch_lens, batch_loss\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        batch_rets, batch_lens, batch_loss = train_one_epoch()\n",
    "        #print(model.layers[3].log_std.numpy())\n",
    "        if i % 50 == 0 or i == epochs-1:\n",
    "            model.save_weights('ckpts/_model_'+str(i), save_format='tf')\n",
    "        \n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "              (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning on Discrete envirionment\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observations (InputLayer)    [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "logits (Dense)               (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "distribution_lambda_18 (Dist ((None,), (None,))        0         \n",
      "=================================================================\n",
      "Total params: 226\n",
      "Trainable params: 226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Layer dense_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "(4109,)\n",
      "epoch:   0 \t loss: 10.844 \t return: 22.091 \t ep_len: 22.091\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print('Trainning on Discrete envirionment')\n",
    "train(env, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(path, obs_dim, act_dim):\n",
    "    model = get_actor(obs_dim, act_dim)\n",
    "    model.load_weights(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, env):\n",
    "    assert isinstance(env.observation_space, Box), \\\n",
    "        \"This is only for continuous observation space\"\n",
    "    assert isinstance(env.action_space, Box), \\\n",
    "        \"This is only for continuous action space\"\n",
    "\n",
    "    env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "\n",
    "    for _ in range(100):\n",
    "        img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        action = model(obs[None])[1][0]\n",
    "        env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-66232f13a401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobs_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mact_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(input_shape)\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                  shape=[int(input_shape[-1]),\n",
    "                                         self.num_outputs])\n",
    "\n",
    "    def call(self, input):\n",
    "        return tf.matmul(input, self.kernel)\n",
    "\n",
    "layer = MyDenseLayer(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "_ = layer(tf.zeros([10, 5])) # Calling the layer `.builds` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([5, 10])]\n"
     ]
    }
   ],
   "source": [
    "print([var.shape for var in layer.trainable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

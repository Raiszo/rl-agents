{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tensorflow_probability import distributions as tfpd, layers as tfpl\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSample(kl.Layer):\n",
    "    def __init__(self, act_dim):\n",
    "        super(GaussianSample, self).__init__()\n",
    "        self.log_std = self.add_weight(\n",
    "            'log_std', shape=(act_dim,),\n",
    "            initializer=tf.keras.initializers.Constant(0),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.normal_dist = tfpl.DistributionLambda(\n",
    "            make_distribution_fn=lambda t: tfpd.Normal(loc=t, scale=tf.exp(self.log_std)),\n",
    "            convert_to_tensor_fn=lambda s: s.sample(),\n",
    "        )\n",
    "    \n",
    "    #def build(self, input_shape):\n",
    "    #    \"\"\"\n",
    "    #    input_shape: might be [None, act_dim]\n",
    "    #    \"\"\"\n",
    "    #    self.log_std = self.add_weight(\n",
    "    #        'log_std', initializer=tf.keras.initializers.Constant(-0.53), \n",
    "    #        shape=(input_shape[1],), dtype='float32', trainable=True\n",
    "    #    )\n",
    "\n",
    "    def call(self, input):\n",
    "        #return tfpd.Normal(loc=input, scale=tf.exp(self.log_std))\n",
    "        return self.normal_dist(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should the output layer have a non linear activation? \n",
    "\n",
    "NO, [spinningup](https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py#L9) uses an identity output activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_actor(obs_dim: int, act_dim: int):\n",
    "    obs = keras.Input(shape=(obs_dim,), name='observations')\n",
    "    x = kl.Dense(32, activation='tanh', name='dense_1')(obs)\n",
    "    x = kl.Dense(act_dim, name='logits')(x)\n",
    "    pi = GaussianSample(act_dim)(x)\n",
    "    \n",
    "    model = keras.Model(inputs=obs,\n",
    "                        outputs=pi)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (distribution_lambda), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'log_std:0' shape=(1,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    }
   ],
   "source": [
    "test_model = get_actor(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.layers[3].log_std.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "[-0.58579165]\n"
     ]
    }
   ],
   "source": [
    "obs = np.array([[ 0.69950885,  0.6432279 , -7.588761  ]])\n",
    "res = test_model(obs)\n",
    "print(res.sample()[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tfpd.Categorical(logits=[[2, 2, 3], [2, -2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5514447, -0.3181754], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.log_prob([2, 2]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_actor(obs_dim: int, act_dim: int):\n",
    "    obs = keras.Input(shape=(obs_dim,), name='observations')\n",
    "    x = kl.Dense(32, activation='tanh', name='dense_1')(obs)\n",
    "    x = kl.Dense(act_dim, name='logits')(x)\n",
    "\n",
    "    pi = tfpl.DistributionLambda(\n",
    "        make_distribution_fn=lambda t: tfpd.Categorical(logits=t),\n",
    "        convert_to_tensor_fn=lambda s: s.sample(),\n",
    "    )(x)\n",
    "    \n",
    "    model = keras.Model(inputs=obs,\n",
    "                        outputs=pi)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = get_discrete_actor(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tf.Tensor([-0.9112284], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "obs = np.array([[ 0.69950885,  0.6432279 , -1.588761  ]])\n",
    "res = test_model(obs)\n",
    "print(res.sample()[0].numpy())\n",
    "print(res.log_prob([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration test model + environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_in_env(model, env):\n",
    "    obs = env.reset()\n",
    "    ret = 0\n",
    "    while True:\n",
    "        act = model(obs[None]).sample()[0].numpy()\n",
    "        obs, rew, done, _ = env.step(act)\n",
    "\n",
    "        ret += rew\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_env = gym.make('CartPole-v0')\n",
    "\n",
    "model = get_discrete_actor(\n",
    "    discrete_env.observation_space.shape[0], \n",
    "    discrete_env.action_space.n\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall return 17.0\n"
     ]
    }
   ],
   "source": [
    "ret = test_model_in_env(model, discrete_env)\n",
    "print(f'Overall return {ret}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt_fn(model, lr=3e-4):\n",
    "    opt = tf.keras.optimizers.Adam(lr)\n",
    "    \n",
    "    #@tf.function\n",
    "    def step_fn(obs_no, act_na, adv_n):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logp = model(obs_no).log_prob(act_na)\n",
    "            # print(logp.shape)\n",
    "            if len(logp.shape) > 1:\n",
    "                logp =  tf.reduce_sum(logp, axis=1)\n",
    "                \n",
    "            weights = tf.stop_gradient(tf.squeeze(adv_n))\n",
    "            \n",
    "            #print(logp.dtype, adv_n.dtype)\n",
    "            loss = tf.reduce_mean(- logp * weights)\n",
    "        \n",
    "        grad = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grad, model.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    return opt, step_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_to_go(rews):\n",
    "    n = len(rews)\n",
    "    rtgs = np.zeros_like(rews)\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "    return rtgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, epochs=200, buffer_size=4096, lr=5e-3):\n",
    "    assert isinstance(env.observation_space, gym.spaces.Box), \\\n",
    "        \"This is only for continuous observation space\"\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    model = None\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        model = get_continuous_actor(obs_dim, env.action_space.shape[0])\n",
    "    else:\n",
    "        model = get_discrete_actor(obs_dim, env.action_space.n)\n",
    "\n",
    "    model.summary()\n",
    "    # keras.utils.plot_model(model, 'multi_output_model.png')\n",
    "    \n",
    "    \n",
    "    opt, step_fn = get_opt_fn(model, lr=lr)\n",
    "    \n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for reward-to-go weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "            # save obs\n",
    "            batch_obs.append(obs.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            act = model(obs[None]).sample()[0].numpy()\n",
    "            obs, rew, done, _ = env.step(act)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a_t|s_t) is reward-to-go from t\n",
    "                batch_weights += list(reward_to_go(ep_rews))\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > buffer_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        batch_loss = step_fn(np.array(batch_obs).astype('float32'), \n",
    "                             np.array(batch_acts).astype('float32'), \n",
    "                             np.array(batch_weights).astype('float32'))\n",
    "        return batch_rets, batch_lens, batch_loss\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        batch_rets, batch_lens, batch_loss = train_one_epoch()\n",
    "        #print(model.layers[3].log_std.numpy())\n",
    "        if i % 50 == 0 or i == epochs-1:\n",
    "            model.save_weights('ckpts/_model_'+str(i), save_format='tf')\n",
    "        \n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "              (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning on Discrete envirionment\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observations (InputLayer)    [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "logits (Dense)               (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "distribution_lambda_2 (Distr ((None,), (None,))        0         \n",
      "=================================================================\n",
      "Total params: 226\n",
      "Trainable params: 226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch:   0 \t loss: 15.140 \t return: 34.754 \t ep_len: 34.754\n",
      "epoch:   1 \t loss: 16.907 \t return: 37.945 \t ep_len: 37.945\n",
      "epoch:   2 \t loss: 14.270 \t return: 35.120 \t ep_len: 35.120\n",
      "epoch:   3 \t loss: 15.838 \t return: 38.925 \t ep_len: 38.925\n",
      "epoch:   4 \t loss: 17.460 \t return: 43.200 \t ep_len: 43.200\n",
      "epoch:   5 \t loss: 19.108 \t return: 45.824 \t ep_len: 45.824\n",
      "epoch:   6 \t loss: 21.125 \t return: 50.390 \t ep_len: 50.390\n",
      "epoch:   7 \t loss: 20.069 \t return: 51.788 \t ep_len: 51.788\n",
      "epoch:   8 \t loss: 18.674 \t return: 50.975 \t ep_len: 50.975\n",
      "epoch:   9 \t loss: 20.309 \t return: 56.452 \t ep_len: 56.452\n",
      "epoch:  10 \t loss: 20.277 \t return: 52.641 \t ep_len: 52.641\n",
      "epoch:  11 \t loss: 21.922 \t return: 57.288 \t ep_len: 57.288\n",
      "epoch:  12 \t loss: 21.639 \t return: 55.459 \t ep_len: 55.459\n",
      "epoch:  13 \t loss: 20.038 \t return: 55.770 \t ep_len: 55.770\n",
      "epoch:  14 \t loss: 21.036 \t return: 59.826 \t ep_len: 59.826\n",
      "epoch:  15 \t loss: 22.147 \t return: 64.031 \t ep_len: 64.031\n",
      "epoch:  16 \t loss: 24.584 \t return: 65.079 \t ep_len: 65.079\n",
      "epoch:  17 \t loss: 22.656 \t return: 62.303 \t ep_len: 62.303\n",
      "epoch:  18 \t loss: 21.834 \t return: 63.338 \t ep_len: 63.338\n",
      "epoch:  19 \t loss: 21.339 \t return: 64.953 \t ep_len: 64.953\n",
      "epoch:  20 \t loss: 26.869 \t return: 73.286 \t ep_len: 73.286\n",
      "epoch:  21 \t loss: 24.157 \t return: 73.286 \t ep_len: 73.286\n",
      "epoch:  22 \t loss: 28.038 \t return: 79.673 \t ep_len: 79.673\n",
      "epoch:  23 \t loss: 30.381 \t return: 87.170 \t ep_len: 87.170\n",
      "epoch:  24 \t loss: 27.849 \t return: 84.102 \t ep_len: 84.102\n",
      "epoch:  25 \t loss: 32.525 \t return: 99.167 \t ep_len: 99.167\n",
      "epoch:  26 \t loss: 32.686 \t return: 98.024 \t ep_len: 98.024\n",
      "epoch:  27 \t loss: 36.525 \t return: 108.816 \t ep_len: 108.816\n",
      "epoch:  28 \t loss: 40.310 \t return: 126.879 \t ep_len: 126.879\n",
      "epoch:  29 \t loss: 40.184 \t return: 124.529 \t ep_len: 124.529\n",
      "epoch:  30 \t loss: 46.817 \t return: 151.214 \t ep_len: 151.214\n",
      "epoch:  31 \t loss: 49.805 \t return: 163.038 \t ep_len: 163.038\n",
      "epoch:  32 \t loss: 47.992 \t return: 146.893 \t ep_len: 146.893\n",
      "epoch:  33 \t loss: 51.408 \t return: 160.346 \t ep_len: 160.346\n",
      "epoch:  34 \t loss: 52.008 \t return: 174.583 \t ep_len: 174.583\n",
      "epoch:  35 \t loss: 54.165 \t return: 185.696 \t ep_len: 185.696\n",
      "epoch:  36 \t loss: 54.936 \t return: 187.682 \t ep_len: 187.682\n",
      "epoch:  37 \t loss: 54.439 \t return: 182.522 \t ep_len: 182.522\n",
      "epoch:  38 \t loss: 54.997 \t return: 191.455 \t ep_len: 191.455\n",
      "epoch:  39 \t loss: 54.482 \t return: 187.455 \t ep_len: 187.455\n",
      "epoch:  40 \t loss: 54.946 \t return: 191.682 \t ep_len: 191.682\n",
      "epoch:  41 \t loss: 53.887 \t return: 190.136 \t ep_len: 190.136\n",
      "epoch:  42 \t loss: 55.298 \t return: 190.045 \t ep_len: 190.045\n",
      "epoch:  43 \t loss: 54.833 \t return: 187.227 \t ep_len: 187.227\n",
      "epoch:  44 \t loss: 53.536 \t return: 186.565 \t ep_len: 186.565\n",
      "epoch:  45 \t loss: 54.978 \t return: 195.714 \t ep_len: 195.714\n",
      "epoch:  46 \t loss: 52.884 \t return: 181.478 \t ep_len: 181.478\n",
      "epoch:  47 \t loss: 54.779 \t return: 191.455 \t ep_len: 191.455\n",
      "epoch:  48 \t loss: 53.308 \t return: 187.864 \t ep_len: 187.864\n",
      "epoch:  49 \t loss: 54.376 \t return: 193.545 \t ep_len: 193.545\n",
      "epoch:  50 \t loss: 54.308 \t return: 197.143 \t ep_len: 197.143\n",
      "epoch:  51 \t loss: 52.182 \t return: 181.043 \t ep_len: 181.043\n",
      "epoch:  52 \t loss: 53.074 \t return: 184.000 \t ep_len: 184.000\n",
      "epoch:  53 \t loss: 53.694 \t return: 195.952 \t ep_len: 195.952\n",
      "epoch:  54 \t loss: 51.426 \t return: 185.870 \t ep_len: 185.870\n",
      "epoch:  55 \t loss: 51.060 \t return: 188.045 \t ep_len: 188.045\n",
      "epoch:  56 \t loss: 51.605 \t return: 186.522 \t ep_len: 186.522\n",
      "epoch:  57 \t loss: 51.007 \t return: 186.273 \t ep_len: 186.273\n",
      "epoch:  58 \t loss: 51.618 \t return: 182.870 \t ep_len: 182.870\n",
      "epoch:  59 \t loss: 49.972 \t return: 181.739 \t ep_len: 181.739\n",
      "epoch:  60 \t loss: 51.650 \t return: 189.955 \t ep_len: 189.955\n",
      "epoch:  61 \t loss: 50.403 \t return: 183.870 \t ep_len: 183.870\n",
      "epoch:  62 \t loss: 51.898 \t return: 190.500 \t ep_len: 190.500\n",
      "epoch:  63 \t loss: 49.661 \t return: 180.130 \t ep_len: 180.130\n",
      "epoch:  64 \t loss: 51.845 \t return: 190.091 \t ep_len: 190.091\n",
      "epoch:  65 \t loss: 51.729 \t return: 187.409 \t ep_len: 187.409\n",
      "epoch:  66 \t loss: 52.024 \t return: 194.045 \t ep_len: 194.045\n",
      "epoch:  67 \t loss: 52.966 \t return: 197.524 \t ep_len: 197.524\n",
      "epoch:  68 \t loss: 52.835 \t return: 195.714 \t ep_len: 195.714\n",
      "epoch:  69 \t loss: 53.368 \t return: 197.571 \t ep_len: 197.571\n",
      "epoch:  70 \t loss: 54.200 \t return: 197.952 \t ep_len: 197.952\n",
      "epoch:  71 \t loss: 53.876 \t return: 198.000 \t ep_len: 198.000\n",
      "epoch:  72 \t loss: 54.210 \t return: 199.905 \t ep_len: 199.905\n",
      "epoch:  73 \t loss: 53.045 \t return: 197.619 \t ep_len: 197.619\n",
      "epoch:  74 \t loss: 54.625 \t return: 198.667 \t ep_len: 198.667\n",
      "epoch:  75 \t loss: 53.566 \t return: 198.524 \t ep_len: 198.524\n",
      "epoch:  76 \t loss: 54.226 \t return: 199.429 \t ep_len: 199.429\n",
      "epoch:  77 \t loss: 55.321 \t return: 199.571 \t ep_len: 199.571\n",
      "epoch:  78 \t loss: 54.001 \t return: 198.810 \t ep_len: 198.810\n",
      "epoch:  79 \t loss: 54.891 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  80 \t loss: 54.590 \t return: 198.762 \t ep_len: 198.762\n",
      "epoch:  81 \t loss: 54.670 \t return: 199.429 \t ep_len: 199.429\n",
      "epoch:  82 \t loss: 54.538 \t return: 199.286 \t ep_len: 199.286\n",
      "epoch:  83 \t loss: 54.892 \t return: 199.952 \t ep_len: 199.952\n",
      "epoch:  84 \t loss: 53.719 \t return: 196.714 \t ep_len: 196.714\n",
      "epoch:  85 \t loss: 54.433 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  86 \t loss: 54.505 \t return: 197.619 \t ep_len: 197.619\n",
      "epoch:  87 \t loss: 54.147 \t return: 197.000 \t ep_len: 197.000\n",
      "epoch:  88 \t loss: 53.717 \t return: 194.591 \t ep_len: 194.591\n",
      "epoch:  89 \t loss: 55.811 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  90 \t loss: 54.483 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  91 \t loss: 55.677 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  92 \t loss: 55.124 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  93 \t loss: 54.413 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  94 \t loss: 55.850 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  95 \t loss: 54.530 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  96 \t loss: 54.653 \t return: 199.810 \t ep_len: 199.810\n",
      "epoch:  97 \t loss: 55.489 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  98 \t loss: 55.239 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  99 \t loss: 53.816 \t return: 198.190 \t ep_len: 198.190\n",
      "epoch: 100 \t loss: 55.188 \t return: 198.333 \t ep_len: 198.333\n",
      "epoch: 101 \t loss: 54.302 \t return: 199.095 \t ep_len: 199.095\n",
      "epoch: 102 \t loss: 54.402 \t return: 198.381 \t ep_len: 198.381\n",
      "epoch: 103 \t loss: 54.414 \t return: 198.524 \t ep_len: 198.524\n",
      "epoch: 104 \t loss: 53.209 \t return: 196.810 \t ep_len: 196.810\n",
      "epoch: 105 \t loss: 54.662 \t return: 198.952 \t ep_len: 198.952\n",
      "epoch: 106 \t loss: 53.902 \t return: 190.909 \t ep_len: 190.909\n",
      "epoch: 107 \t loss: 52.929 \t return: 196.095 \t ep_len: 196.095\n",
      "epoch: 108 \t loss: 52.594 \t return: 188.955 \t ep_len: 188.955\n",
      "epoch: 109 \t loss: 53.610 \t return: 197.524 \t ep_len: 197.524\n",
      "epoch: 110 \t loss: 54.693 \t return: 199.238 \t ep_len: 199.238\n",
      "epoch: 111 \t loss: 53.505 \t return: 194.273 \t ep_len: 194.273\n",
      "epoch: 112 \t loss: 52.830 \t return: 196.286 \t ep_len: 196.286\n",
      "epoch: 113 \t loss: 52.973 \t return: 197.381 \t ep_len: 197.381\n",
      "epoch: 114 \t loss: 53.347 \t return: 197.048 \t ep_len: 197.048\n",
      "epoch: 115 \t loss: 53.241 \t return: 198.476 \t ep_len: 198.476\n",
      "epoch: 116 \t loss: 53.948 \t return: 198.524 \t ep_len: 198.524\n",
      "epoch: 117 \t loss: 52.966 \t return: 196.381 \t ep_len: 196.381\n",
      "epoch: 118 \t loss: 53.416 \t return: 194.955 \t ep_len: 194.955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 119 \t loss: 54.017 \t return: 199.619 \t ep_len: 199.619\n",
      "epoch: 120 \t loss: 53.719 \t return: 199.857 \t ep_len: 199.857\n",
      "epoch: 121 \t loss: 53.688 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 122 \t loss: 53.782 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 123 \t loss: 54.082 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 124 \t loss: 52.703 \t return: 188.136 \t ep_len: 188.136\n",
      "epoch: 125 \t loss: 53.498 \t return: 196.476 \t ep_len: 196.476\n",
      "epoch: 126 \t loss: 52.741 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 127 \t loss: 53.546 \t return: 199.571 \t ep_len: 199.571\n",
      "epoch: 128 \t loss: 52.817 \t return: 193.636 \t ep_len: 193.636\n",
      "epoch: 129 \t loss: 53.086 \t return: 199.571 \t ep_len: 199.571\n",
      "epoch: 130 \t loss: 53.106 \t return: 195.524 \t ep_len: 195.524\n",
      "epoch: 131 \t loss: 52.385 \t return: 199.762 \t ep_len: 199.762\n",
      "epoch: 132 \t loss: 52.342 \t return: 199.143 \t ep_len: 199.143\n",
      "epoch: 133 \t loss: 52.566 \t return: 199.905 \t ep_len: 199.905\n",
      "epoch: 134 \t loss: 50.664 \t return: 189.182 \t ep_len: 189.182\n",
      "epoch: 135 \t loss: 52.336 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 136 \t loss: 51.726 \t return: 197.524 \t ep_len: 197.524\n",
      "epoch: 137 \t loss: 52.162 \t return: 196.190 \t ep_len: 196.190\n",
      "epoch: 138 \t loss: 51.026 \t return: 191.682 \t ep_len: 191.682\n",
      "epoch: 139 \t loss: 50.728 \t return: 196.238 \t ep_len: 196.238\n",
      "epoch: 140 \t loss: 50.099 \t return: 196.286 \t ep_len: 196.286\n",
      "epoch: 141 \t loss: 51.249 \t return: 197.286 \t ep_len: 197.286\n",
      "epoch: 142 \t loss: 49.084 \t return: 190.455 \t ep_len: 190.455\n",
      "epoch: 143 \t loss: 50.359 \t return: 190.273 \t ep_len: 190.273\n",
      "epoch: 144 \t loss: 48.392 \t return: 191.500 \t ep_len: 191.500\n",
      "epoch: 145 \t loss: 50.264 \t return: 189.455 \t ep_len: 189.455\n",
      "epoch: 146 \t loss: 49.713 \t return: 194.591 \t ep_len: 194.591\n",
      "epoch: 147 \t loss: 50.166 \t return: 195.429 \t ep_len: 195.429\n",
      "epoch: 148 \t loss: 51.377 \t return: 198.286 \t ep_len: 198.286\n",
      "epoch: 149 \t loss: 51.703 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 150 \t loss: 50.939 \t return: 199.190 \t ep_len: 199.190\n",
      "epoch: 151 \t loss: 50.964 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 152 \t loss: 51.897 \t return: 197.000 \t ep_len: 197.000\n",
      "epoch: 153 \t loss: 51.796 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 154 \t loss: 51.930 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 155 \t loss: 52.098 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 156 \t loss: 52.433 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 157 \t loss: 51.972 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 158 \t loss: 52.552 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 159 \t loss: 51.438 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 160 \t loss: 53.147 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 161 \t loss: 52.886 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 162 \t loss: 53.220 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 163 \t loss: 52.458 \t return: 198.238 \t ep_len: 198.238\n",
      "epoch: 164 \t loss: 52.685 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 165 \t loss: 52.641 \t return: 196.524 \t ep_len: 196.524\n",
      "epoch: 166 \t loss: 52.783 \t return: 198.095 \t ep_len: 198.095\n",
      "epoch: 167 \t loss: 53.546 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 168 \t loss: 53.947 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 169 \t loss: 53.116 \t return: 199.429 \t ep_len: 199.429\n",
      "epoch: 170 \t loss: 53.795 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 171 \t loss: 53.147 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 172 \t loss: 52.336 \t return: 195.000 \t ep_len: 195.000\n",
      "epoch: 173 \t loss: 53.326 \t return: 199.190 \t ep_len: 199.190\n",
      "epoch: 174 \t loss: 52.849 \t return: 199.333 \t ep_len: 199.333\n",
      "epoch: 175 \t loss: 53.662 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 176 \t loss: 53.676 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 177 \t loss: 53.000 \t return: 197.429 \t ep_len: 197.429\n",
      "epoch: 178 \t loss: 52.960 \t return: 196.429 \t ep_len: 196.429\n",
      "epoch: 179 \t loss: 53.841 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 180 \t loss: 54.337 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 181 \t loss: 53.595 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 182 \t loss: 52.582 \t return: 196.810 \t ep_len: 196.810\n",
      "epoch: 183 \t loss: 53.985 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 184 \t loss: 53.813 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 185 \t loss: 53.439 \t return: 197.476 \t ep_len: 197.476\n",
      "epoch: 186 \t loss: 54.083 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 187 \t loss: 54.014 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 188 \t loss: 52.571 \t return: 196.000 \t ep_len: 196.000\n",
      "epoch: 189 \t loss: 53.711 \t return: 194.545 \t ep_len: 194.545\n",
      "epoch: 190 \t loss: 52.664 \t return: 191.045 \t ep_len: 191.045\n",
      "epoch: 191 \t loss: 53.093 \t return: 195.857 \t ep_len: 195.857\n",
      "epoch: 192 \t loss: 53.850 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 193 \t loss: 53.933 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 194 \t loss: 52.732 \t return: 198.190 \t ep_len: 198.190\n",
      "epoch: 195 \t loss: 53.789 \t return: 197.619 \t ep_len: 197.619\n",
      "epoch: 196 \t loss: 53.897 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 197 \t loss: 53.531 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 198 \t loss: 53.520 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch: 199 \t loss: 53.107 \t return: 200.000 \t ep_len: 200.000\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print('Trainning on Discrete envirionment')\n",
    "train(env, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning on Discrete envirionment\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observations (InputLayer)    [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "logits (Dense)               (None, 3)                 99        \n",
      "_________________________________________________________________\n",
      "distribution_lambda_1 (Distr ((None,), (None,))        0         \n",
      "=================================================================\n",
      "Total params: 195\n",
      "Trainable params: 195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch:   0 \t loss: -109.427 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:   1 \t loss: -109.587 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:   2 \t loss: -109.177 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:   3 \t loss: -109.412 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:   4 \t loss: -108.888 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:   5 \t loss: -109.328 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:   6 \t loss: -108.957 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:   7 \t loss: -109.401 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:   8 \t loss: -109.623 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:   9 \t loss: -109.358 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  10 \t loss: -109.581 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  11 \t loss: -109.791 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  12 \t loss: -109.593 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  13 \t loss: -109.267 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  14 \t loss: -109.244 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  15 \t loss: -109.222 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  16 \t loss: -109.236 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  17 \t loss: -109.685 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  18 \t loss: -109.284 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  19 \t loss: -109.363 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  20 \t loss: -108.661 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  21 \t loss: -108.857 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  22 \t loss: -108.613 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  23 \t loss: -109.306 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  24 \t loss: -108.293 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  25 \t loss: -108.149 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  26 \t loss: -108.910 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  27 \t loss: -108.900 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  28 \t loss: -108.234 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  29 \t loss: -108.315 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  30 \t loss: -108.598 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  31 \t loss: -108.287 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  32 \t loss: -108.914 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  33 \t loss: -108.550 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  34 \t loss: -107.801 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  35 \t loss: -107.648 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  36 \t loss: -105.718 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  37 \t loss: -106.480 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  38 \t loss: -106.212 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  39 \t loss: -106.678 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  40 \t loss: -106.065 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  41 \t loss: -106.031 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  42 \t loss: -108.011 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  43 \t loss: -106.271 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  44 \t loss: -106.694 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  45 \t loss: -107.169 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  46 \t loss: -106.121 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  47 \t loss: -105.787 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  48 \t loss: -107.087 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  49 \t loss: -107.250 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  50 \t loss: -108.027 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  51 \t loss: -108.142 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  52 \t loss: -108.127 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  53 \t loss: -108.514 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  54 \t loss: -109.126 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  55 \t loss: -109.109 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  56 \t loss: -109.256 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  57 \t loss: -109.471 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  58 \t loss: -109.656 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  59 \t loss: -109.385 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  60 \t loss: -109.244 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  61 \t loss: -108.200 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  62 \t loss: -108.433 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  63 \t loss: -107.451 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  64 \t loss: -108.228 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  65 \t loss: -106.753 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  66 \t loss: -106.830 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  67 \t loss: -104.847 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  68 \t loss: -106.078 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  69 \t loss: -104.171 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  70 \t loss: -103.328 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  71 \t loss: -103.032 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  72 \t loss: -103.820 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  73 \t loss: -102.362 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  74 \t loss: -102.730 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  75 \t loss: -102.192 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  76 \t loss: -102.571 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  77 \t loss: -102.380 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  78 \t loss: -102.540 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  79 \t loss: -103.380 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  80 \t loss: -102.822 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  81 \t loss: -104.818 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  82 \t loss: -105.104 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  83 \t loss: -105.804 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  84 \t loss: -105.722 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  85 \t loss: -105.949 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  86 \t loss: -105.112 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  87 \t loss: -105.613 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  88 \t loss: -105.789 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  89 \t loss: -104.469 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  90 \t loss: -102.825 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  91 \t loss: -104.496 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  92 \t loss: -103.824 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  93 \t loss: -103.366 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  94 \t loss: -103.890 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  95 \t loss: -104.509 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  96 \t loss: -102.579 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  97 \t loss: -103.084 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  98 \t loss: -103.312 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch:  99 \t loss: -102.694 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 100 \t loss: -104.308 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 101 \t loss: -104.336 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 102 \t loss: -105.145 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 103 \t loss: -105.163 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 104 \t loss: -103.615 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 105 \t loss: -104.611 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 106 \t loss: -105.516 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 107 \t loss: -105.671 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 108 \t loss: -105.343 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 109 \t loss: -104.318 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 110 \t loss: -105.113 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 111 \t loss: -105.805 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 112 \t loss: -106.281 \t return: -200.000 \t ep_len: 200.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 113 \t loss: -106.041 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 114 \t loss: -107.550 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 115 \t loss: -106.850 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 116 \t loss: -106.514 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 117 \t loss: -106.711 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 118 \t loss: -106.855 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 119 \t loss: -105.883 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 120 \t loss: -106.272 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 121 \t loss: -106.862 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 122 \t loss: -107.724 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 123 \t loss: -107.925 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 124 \t loss: -108.369 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 125 \t loss: -107.980 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 126 \t loss: -108.434 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 127 \t loss: -108.142 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 128 \t loss: -108.754 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 129 \t loss: -108.306 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 130 \t loss: -108.130 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 131 \t loss: -108.312 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 132 \t loss: -109.167 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 133 \t loss: -108.438 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 134 \t loss: -108.563 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 135 \t loss: -108.307 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 136 \t loss: -108.668 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 137 \t loss: -109.149 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 138 \t loss: -108.776 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 139 \t loss: -108.566 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 140 \t loss: -108.075 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 141 \t loss: -107.529 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 142 \t loss: -108.136 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 143 \t loss: -107.357 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 144 \t loss: -107.284 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 145 \t loss: -106.924 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 146 \t loss: -107.422 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 147 \t loss: -108.235 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 148 \t loss: -106.753 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 149 \t loss: -105.654 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 150 \t loss: -105.368 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 151 \t loss: -106.378 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 152 \t loss: -106.592 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 153 \t loss: -106.977 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 154 \t loss: -105.452 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 155 \t loss: -107.029 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 156 \t loss: -106.229 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 157 \t loss: -106.291 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 158 \t loss: -105.643 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 159 \t loss: -107.126 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 160 \t loss: -107.557 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 161 \t loss: -106.790 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 162 \t loss: -107.873 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 163 \t loss: -107.679 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 164 \t loss: -108.637 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 165 \t loss: -109.102 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 166 \t loss: -109.505 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 167 \t loss: -109.682 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 168 \t loss: -110.086 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 169 \t loss: -110.074 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 170 \t loss: -109.952 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 171 \t loss: -110.060 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 172 \t loss: -110.220 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 173 \t loss: -109.955 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 174 \t loss: -109.699 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 175 \t loss: -109.491 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 176 \t loss: -109.881 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 177 \t loss: -109.905 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 178 \t loss: -109.375 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 179 \t loss: -109.774 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 180 \t loss: -109.828 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 181 \t loss: -109.832 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 182 \t loss: -109.654 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 183 \t loss: -109.910 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 184 \t loss: -109.499 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 185 \t loss: -109.884 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 186 \t loss: -109.696 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 187 \t loss: -109.984 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 188 \t loss: -109.974 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 189 \t loss: -109.064 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 190 \t loss: -109.076 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 191 \t loss: -109.330 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 192 \t loss: -109.668 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 193 \t loss: -109.353 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 194 \t loss: -109.412 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 195 \t loss: -109.338 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 196 \t loss: -109.583 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 197 \t loss: -109.723 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 198 \t loss: -109.178 \t return: -200.000 \t ep_len: 200.000\n",
      "epoch: 199 \t loss: -109.521 \t return: -200.000 \t ep_len: 200.000\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "print('Trainning on Discrete envirionment')\n",
    "train(env, epochs=200, lr=5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(path, obs_dim, act_dim):\n",
    "    model = get_actor(obs_dim, act_dim)\n",
    "    model.load_weights(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, env):\n",
    "    assert isinstance(env.observation_space, Box), \\\n",
    "        \"This is only for continuous observation space\"\n",
    "    assert isinstance(env.action_space, Box), \\\n",
    "        \"This is only for continuous action space\"\n",
    "\n",
    "    env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "\n",
    "    for _ in range(100):\n",
    "        img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        action = model(obs[None]).loc[0]\n",
    "        env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-66232f13a401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobs_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mact_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(input_shape)\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                  shape=[int(input_shape[-1]),\n",
    "                                         self.num_outputs])\n",
    "\n",
    "    def call(self, input):\n",
    "        return tf.matmul(input, self.kernel)\n",
    "\n",
    "layer = MyDenseLayer(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "_ = layer(tf.zeros([10, 5])) # Calling the layer `.builds` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([5, 10])]\n"
     ]
    }
   ],
   "source": [
    "print([var.shape for var in layer.trainable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
